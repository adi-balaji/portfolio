<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advaith Balaji | Portfolio</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    
    <link href="css/style.css" rel="stylesheet" type="text/css"/>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Geologica:wght@100..900&family=Manrope:wght@200..800&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

    
        <div class="navbar-logo-left-6">

            <div class="navbarcontainer">
                <div class="navbarcontent">
                    <div class="navbarleftside">

                        <!--
                        <div class="navbarbrand">
                            <img src="images/mylogo2-1.png" loading="lazy" width="Auto" height="50" alt="" class="logo"/></div>
                         -->
                         
                            <a href = "index.html">
                            <div class="text-24">Advaith Balaji</div>
                        </a>
                        </div>

                        <button class="hamburger" id="hamburger">
                            <span></span>
                            <span></span>
                            <span></span>
                        </button>


                        <nav class="navbarmenu" id = "navMenu">
                                <a href="index.html" class="navlink">
                                    
                                    <div>home</div>
                                </a>
                                <a href="projects.html" class="navlink">
                                    <div>projects</div>
                                </a>
                                <a href="resume.html" class="navlink">
                                    <div>resume</div>
                                </a>
                            
                            </nav>
                        </div>
                      
                        
                    </div>
                </div>
</head>

<body>
<div class = "projectspage">

  <div class="projectcontainer fade-in-section">
    <h4 class="projectname">Semantic Reasoning for Object Search in Farms</h4>
    <div class="projectpics" id="pp1">
      <video controls width="500" autoplay loop muted>
        <source src="images/hottest_object.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  
    <p class="projectdescription">
      To create a more sustainable future, we need to work towards robots that can assist in farms and gardens to reduce the mental and physical workload of farm workers in a time of increasing crop shortages. We tackle the problem of intelligent object retrieval in a farm environment, providing a method that allows a robot to semantically reason about the location of an unseen goal object among a set of previously seen objects in the environment using a Large Language Model (LLM). We leverage object-to-object semantic relationships to best determine the best location to visit in order to most likely find our goal object. We deployed our system on the Boston Dynamics Spot Robot and found a success rate of 79%, with more improvements on the way. Research paper and website coming soon!
    </p>
  </div>

  <div class = "projectcontainer fade-in-section">
    <h4 class = "projectname">Open-Vocabulary Object Localization with OWL-ViT and FAST-SAM</h4>
  <div class = "projectpics" id = "pp1">
    <img src = "images/ov_object_loc.gif" alt = ""> <br>
    </div>
    
    <p class = "projectdescription"> Since open-vocabulary detectors suffer from noisy outputs when presented with slightly cluttered scenes, we combine the open-vocabulary detector with a probabilistic filter to calculate better object state estmates to use for robotic grasping. In this case, we combined OWL-ViT and FAST Segment-Anything and leveraged fast CUDA computations to calculate multple segmentation masks of an object given just a text prompt, and then combined the estimates using a probabilistic filter, resulting in a highly reliable and accurate object tracking systems that robots can use for open-vocabulary grasping!</p>
    
  
  </div>

  <div class = "projectcontainer fade-in-section">
    <h4 class = "projectname">GrapeRob: A Grape Localization Pipeline for Automated Robotic Harvesting</h4>
  <div class = "projectpics" id = "pp2">
    <img src = "images/0418.gif" alt = "Demonstration of Grape Harvesting Robot">
    <img src = "images/masking_eg2.webp" alt = ""> 
    </div>

    <p class = "projectdescription">A vision pipeline for grape state estimation to support automated robotic harvesting. This pipeline consists of a grape bunch and stem segmentation model built using PyTorch and the WGISD dataset, and a 3D reconstruction algorithm that combines the grape masks and a depth map to reconstruct a grape point cloud for pose estimation. Read more about it on the <a href="https://deeprob.org/w24/reports/graperob/">project website!</a></p>

  </div>



  <div class = "projectcontainer fade-in-section">
      <h4 class = "projectname">Crosswalk Buddy</h4>
  <div class = "projectpics" id = "pp3">
      <img src = "images/simpic.webp" alt = "">
      <img src = "images/kalman_ped_short.gif" alt = ""> 
      </div>
      
      <p class = "projectdescription">Crosswalk Buddy is an independent research project under UM Robotics with the goal of developing a robot that will increase safety in pedestrian spaces. The aim of this robot is to increase driver visibility of pedestrians in low visibility scenarios. There are two phases - HRI Research and Autonomy Software Development. We developed a simulation of the robot in a city environment to construct human trials to gauge the best proximities and positions of the robot that create a comfortable experience for the pedestrian. Parallely, we have been developing the autonomy stack for a real robot platform. Above, we use YOLOv3 and simple 1D Kalman Filtering to construct a vision system for pedestrian state estimation.</p>
      
      
      </div>

  </div>





</body>

<script src="js/script.js">
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    // Function to set up the IntersectionObserver
    function observeSections() {
      const observer = new IntersectionObserver((entries, observer) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add('is-visible');
            observer.unobserve(entry.target);
          }
        });
      }, {
        threshold: 0.1 // Adjust if needed; value between 0 and 1, where 1 is fully visible
      });
      
      document.querySelectorAll('.projectcontainer').forEach(section => {
        observer.observe(section);
      });
    }
    
    // Start observing project containers for intersection changes
    observeSections();
  });
    </script>
    