<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advaith Balaji | Portfolio</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    
    <link href="css/style.css" rel="stylesheet" type="text/css"/>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Geologica:wght@100..900&family=Manrope:wght@200..800&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

    
        <div class="navbar-logo-left-6">

            <div class="navbarcontainer">
                <div class="navbarcontent">
                    <div class="navbarleftside">

                        <!--
                        <div class="navbarbrand">
                            <img src="images/mylogo2-1.png" loading="lazy" width="Auto" height="50" alt="" class="logo"/></div>
                         -->
                         
                            <a href = "index.html">
                            <div class="text-24">Advaith Balaji</div>
                        </a>
                        </div>

                        <button class="hamburger" id="hamburger">
                            <span></span>
                            <span></span>
                            <span></span>
                        </button>


                        <nav class="navbarmenu" id = "navMenu">
                                <a href="index.html" class="navlink">
                                    
                                    <div>home</div>
                                </a>
                                <a href="projects.html" class="navlink">
                                    <div>projects</div>
                                </a>
                                <a href="resume.html" class="navlink">
                                    <div>resume</div>
                                </a>
                            
                            </nav>
                        </div>
                      
                        
                    </div>
                </div>
</head>

<body>
<div class = "projectspage">

  <div class="projectcontainer fade-in-section">
    <h4 class="projectname">OVAL-Grasp: Open-Vocabulary Affordance Localization for Task-Oriented Grasping</h4>
    <div class="projectpics" id="pp1">
      <video controls width="80%" autoplay loop muted>
        <source src="images/ovalgrasp_robot_demo_vid_compress.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  
    <p class="projectdescription">
      <strong><em>Accepted to the 2025 Internation Symposium on Experimental Robotics (ISER 2025)!</em></strong><br>
      Robots often struggle with task-oriented grasps in unstructured environments. OVAL-Grasp is a zero-shot approach leveraging large-language models (LLMs) and vision-language models (VLMs) to identify and grasp object parts based on a given task. It uses RGB images and task prompts to segment actionable regions and generate heatmaps for grasping. Evaluated on 20 household objects across 3 tasks, OVAL-Grasp achieved a 95% success rate in part identification and 78.3% in actionable area grasping with the Fetch robot. It also performs well under occlusions and cluttered scenes, showcasing robust task-oriented grasping capabilities.
    </p>
  </div>

  <div class="projectcontainer fade-in-section">
    <h4 class="projectname">Language-Guided Object Search in Agricultural Environments</h4>
    <div class="projectpics" id="pp1">
      <video controls width="80%" autoplay loop muted>
        <source src="images/edited_losae_clip.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  
    <p class="projectdescription">
      <strong><em>Presented at the 2025 IEEE International Conference on Robotics & Automation (ICRA 2025)! </em></strong><br>
      <a href="https://arxiv.org/abs/2503.01068" target="_blank">[arxiv]</a>
      To create a more sustainable future, we need to work towards robots that can assist in farms and gardens to reduce the mental and physical workload of farm workers in a time of increasing crop shortages. We tackle the problem of intelligent object retrieval in a farm environment, providing a method that allows a robot to semantically reason about the location of an unseen goal object among a set of previously seen objects in the environment using a Large Language Model (LLM). We leverage object-to-object semantic relationships to best determine the best location to visit in order to most likely find our goal object. We deployed our system on the Boston Dynamics Spot Robot and found an object search success rate of 79%.
    </p>
  </div>




  <div class="projectcontainer fade-in-section">
    <h4 class="projectname">3D Object Localiztion with Signed Distance Fields (SDFs)</h4>
    <div class="projectpics" id="pp1">
      <video controls width="80%" autoplay loop muted>
        <source src="images/sdf_loc.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  
    <p class="projectdescription">
      This project presents a robust method for localizing objects in 3D space using Signed Distance Fields (SDFs). SDFs represent an object's surface as a volumetric field where each point's value corresponds to its signed distance from the surface, enabling efficient geometric and normal-based reasoning. Given a point cloud of an object and its corresponding SDF, the approach estimates the object's 6DOF pose (rotation and translation) by minimizing both the distances to the SDF surface and the angle between surface normals derived from the SDF gradients and the point cloud. The method is evaluated on objects from the YCB dataset, demonstrating high accuracy on full and partial point cloud views, even for symmetric and complex geometries.
    </p>
  </div>







  <div class = "projectcontainer fade-in-section">
    <h4 class = "projectname">GrapeRob: A Grape Localization Pipeline for Automated Robotic Harvesting</h4>
  <div class = "projectpics" id = "pp2">
    <img src = "images/0418.gif" alt = "Demonstration of Grape Harvesting Robot">
    <img src = "images/masking_eg2.webp" alt = ""> 
    </div>

    <p class = "projectdescription">
      <strong><em>Presented at the 2024 Michigan AI Symposium!</em></strong><br>
      A vision pipeline for grape state estimation to support automated robotic harvesting. This pipeline consists of a grape bunch and stem segmentation model built using PyTorch and the WGISD dataset, and a 3D reconstruction algorithm that combines the grape masks and a depth map to reconstruct a grape point cloud for pose estimation. Read more about it on the <a href="https://deeprob.org/w24/reports/graperob/">project website!</a></p>

  </div>
  




  <div class = "projectcontainer fade-in-section">
    <h4 class = "projectname">Open-Vocabulary Object Localization with OWL-ViT and FAST-SAM</h4>
  <div class = "projectpics" id = "pp1">
    <img src = "images/ov_object_loc.gif" alt = ""> <br>
    </div>
    
    <p class = "projectdescription"> Since open-vocabulary detectors suffer from noisy outputs when presented with slightly cluttered scenes, we combine the open-vocabulary detector with a probabilistic filter to calculate better object state estmates to use for robotic grasping. In this case, we combined OWL-ViT and FAST Segment-Anything and leveraged fast CUDA computations to calculate multple segmentation masks of an object given just a text prompt, and then combined the estimates using a probabilistic filter, resulting in a highly reliable and accurate object tracking systems that robots can use for open-vocabulary grasping!</p>
    
  
  </div>




  <div class = "projectcontainer fade-in-section">
      <h4 class = "projectname">Crosswalk Buddy</h4>
  <div class = "projectpics" id = "pp3">
      <img src = "images/simpic.webp" alt = "">
      <img src = "images/kalman_ped_short.gif" alt = ""> 
      </div>
      
      <p class = "projectdescription">
        <strong><em>Presented at the 2024 Michigan Robotics Undergraduate Research Symposium!</em></strong><br>
        Crosswalk Buddy is an independent research project under UM Robotics with the goal of developing a robot that will increase safety in pedestrian spaces. The aim of this robot is to increase driver visibility of pedestrians in low visibility scenarios. There are two phases - HRI Research and Autonomy Software Development. We developed a simulation of the robot in a city environment to construct human trials to gauge the best proximities and positions of the robot that create a comfortable experience for the pedestrian. Parallely, we have been developing the autonomy stack for a real robot platform. Above, we use YOLOv3 and simple 1D Kalman Filtering to construct a vision system for pedestrian state estimation.</p>
      
      
      </div>

  </div>





</body>

<script src="js/script.js">
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    // Function to set up the IntersectionObserver
    function observeSections() {
      const observer = new IntersectionObserver((entries, observer) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add('is-visible');
            observer.unobserve(entry.target);
          }
        });
      }, {
        threshold: 0.1 // Adjust if needed; value between 0 and 1, where 1 is fully visible
      });
      
      document.querySelectorAll('.projectcontainer').forEach(section => {
        observer.observe(section);
      });
    }
    
    // Start observing project containers for intersection changes
    observeSections();
  });
    </script>
    